{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the seed of an existing image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from training.generator import Generator\n",
    "from training import utils\n",
    "from training.settings import *\n",
    "\n",
    "utils.reset_rand()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_gpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/faces_256.pt'\n",
    "\n",
    "model = Generator().to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join('..', MODEL_PATH), map_location = DEVICE))\n",
    "model.eval()\n",
    "model.requires_grad_(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean W image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = model.generate_one(0.0)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = 'test.png'\t\t# Path to the image to project\n",
    "SAVE_DIR = 'projector'\t\t# Directory to save the images to\n",
    "NB_STEPS = 12_000\t\t\t# Number of steps to use for projection\n",
    "MAX_LEARNING_RATE = 0.1\t\t# Maximum learning rate to use\n",
    "WARMUP_STEPS = 500\t\t\t# Number of steps to use for warmup\n",
    "COOLDOWN_STEPS = 500\t\t# Number of steps to use for cooldown\n",
    "W_NOISE_SCALE = 0.1\t\t\t# The scale of the noise on W\n",
    "W_NOISE_STEPS = 7_000\t\t# Number of steps using noise on W\n",
    "NOISE_REG_STRENGTH = 100\t# Strength of the noise regularization\n",
    "NB_SAVED_IMAGES = 1_000\t\t# Number of images to save"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the image and VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert = transforms.Compose([\n",
    "\ttransforms.Resize(IMAGE_SIZE, interpolation = transforms.InterpolationMode.LANCZOS),\n",
    "\ttransforms.CenterCrop(IMAGE_SIZE),\n",
    "\ttransforms.ToTensor()\n",
    "])\n",
    "\n",
    "file = os.path.join('..', IMAGE_PATH)\n",
    "\n",
    "if NB_CHANNELS == 1:\n",
    "\ttarget = Image.open(file).convert('L')\n",
    "elif NB_CHANNELS <= 3:\n",
    "\ttarget = Image.open(file).convert('RGB')\n",
    "else:\n",
    "\ttarget = Image.open(file).convert('RGBA')\n",
    "\n",
    "target = convert(target) * 2.0 - 1.0\n",
    "\n",
    "if NB_CHANNELS == 2:\n",
    "\ttarget = target[:2]\n",
    "\n",
    "target = target.to(DEVICE).unsqueeze(0).detach().requires_grad_(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = tv.models.vgg16(weights = tv.models.VGG16_Weights.DEFAULT).to(DEVICE)\n",
    "vgg16_model = nn.Sequential(*list(vgg16_model.features.children()))\n",
    "vgg16_model.eval()\n",
    "vgg16_model.requires_grad_(False)\n",
    "\n",
    "vgg16_transform = tv.models.VGG16_Weights.DEFAULT.transforms()\n",
    "\n",
    "\n",
    "def vgg16(x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "\tx = (x + 1.0) / 2.0\n",
    "\tx = vgg16_transform(x)\n",
    "\tx = vgg16_model(x)\n",
    "\n",
    "\treturn x.requires_grad_(True)\n",
    "\n",
    "\n",
    "target_features = vgg16(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = model.gen_w(MEAN_W_SAMPLES)\n",
    "w_mean = ws.mean(0, keepdims = True).detach().requires_grad_(False)\n",
    "w_std = ((ws - w_mean).square().sum() / MEAN_W_SAMPLES).sqrt().item()\n",
    "\n",
    "w = w_mean.clone().requires_grad_(True)\n",
    "noise = model.gen_noise(1)\n",
    "noise = [n.requires_grad_(True) for n in noise]\n",
    "\n",
    "optimizer = torch.optim.Adam([w] + noise, lr = 0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(NB_STEPS):\n",
    "\n",
    "\toptimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "\tlr_start = min(1.0, step / WARMUP_STEPS)\n",
    "\tlr_end = min(1.0, (NB_STEPS - step) / COOLDOWN_STEPS)\n",
    "\tlr_end = 0.5 - 0.5 * np.cos(lr_end * np.pi)\n",
    "\tlr = LEARNING_RATE * lr_start * lr_end\n",
    "\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\tparam_group['lr'] = lr\n",
    "\n",
    "\tw_noise_scale = w_std * W_NOISE_SCALE * max(0.0, 1.0 - step / W_NOISE_STEPS) ** 2\n",
    "\tw_noise = w_noise_scale * torch.randn_like(w)\n",
    "\n",
    "\tgen_image = model.synthesis(w + w_noise, noise)\n",
    "\tgen_features = vgg16(gen_image)\n",
    "\n",
    "\tpixel_loss = (gen_image - target).square().mean()\n",
    "\tgen_temp = nn.functional.interpolate(gen_image, scale_factor = 0.5, mode = 'bilinear')\n",
    "\ttarget_temp = nn.functional.interpolate(target, scale_factor = 0.5, mode = 'bilinear')\n",
    "\n",
    "\twhile True:\n",
    "\n",
    "\t\tpixel_loss = pixel_loss + (gen_temp - target_temp).square().mean()\n",
    "\t\tgen_temp = nn.functional.interpolate(gen_temp, scale_factor = 0.5, mode = 'bilinear')\n",
    "\t\ttarget_temp = nn.functional.interpolate(target_temp, scale_factor = 0.5, mode = 'bilinear')\n",
    "\n",
    "\t\tif gen_temp.shape[2] == 1:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tfeatures_loss = (gen_features - target_features).square().mean()\n",
    "\tnoise_reg = 0.0\n",
    "\n",
    "\tfor n in noise:\n",
    "\t\twhile True:\n",
    "\n",
    "\t\t\tnoise_reg = noise_reg + (n * torch.roll(n, shifts = 1, dims = 2)).mean().square()\n",
    "\t\t\tnoise_reg = noise_reg + (n * torch.roll(n, shifts = 1, dims = 3)).mean().square()\n",
    "\t\t\tn = nn.functional.avg_pool2d(n, kernel_size = 2)\n",
    "\n",
    "\t\t\tif n.shape[2] < 8:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\tloss = pixel_loss + math.log2(IMAGE_SIZE) * features_loss + NOISE_REG_STRENGTH * noise_reg\n",
    "\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\n",
    "\t\tfor n in noise:\n",
    "\t\t\tn.sub(n.mean())\n",
    "\t\t\tn.mul(n.square().mean().rsqrt())\n",
    "\n",
    "\t\tif not os.path.exists(os.path.join('..', SAVE_DIR)):\n",
    "\t\t\tos.makedirs(os.path.join('..', SAVE_DIR))\n",
    "\n",
    "\t\tif step % (NB_STEPS // NB_SAVED_IMAGES) == 0:\n",
    "\t\t\tsave_image = utils.denormalize(model.synthesis(w, noise).squeeze(0))\n",
    "\t\t\tImage.fromarray(save_image).save(os.path.join('..', SAVE_DIR, f'{step // (NB_STEPS // NB_SAVED_IMAGES)}.png'))\n",
    "\t\t\tImage.fromarray(save_image).save(os.path.join('..', 'projector.png'))\n",
    "\n",
    "\t\tprint(f'Steps: {step:,} / {NB_STEPS}  ||  Pixel Loss: {pixel_loss.item():.4f} | Features Loss: {math.log2(IMAGE_SIZE) * features_loss.item():.4f} | ' + \\\n",
    "\t\t\tf'Noise Regularisation: {NOISE_REG_STRENGTH * noise_reg.item():.4f}          ', end = '\\r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
