{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the seed of an existing image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from training.generator import Generator\n",
    "from training import utils\n",
    "from training.settings import *\n",
    "\n",
    "utils.reset_rand()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_gpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/faces_256.pt'\n",
    "\n",
    "model = Generator().to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join('..', MODEL_PATH)))\n",
    "model.eval()\n",
    "model.requires_grad_(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean W image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = model.generate_one(0.0)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = 'test.png'\t\t\t# Path to the image to project\n",
    "SAVE_DIR = 'projector'\t\t\t# Directory to save the images to\n",
    "NB_STEPS = 1_000\t\t\t\t# Number of steps to use for projection\n",
    "MAX_LEARNING_RATE = 0.1\t\t\t# Maximum learning rate to use\n",
    "WARMUP_STEPS = 50\t\t\t\t# Number of steps to use for warmup\n",
    "COOLDOWN_STEPS = 250\t\t\t# Number of steps to use for cooldown\n",
    "W_NOISE_SCALE = 0.05\t\t\t# The scale of the noise on W\n",
    "W_NOISE_STEPS = 750\t\t\t\t# Number of steps using noise on W\n",
    "NOISE_REG_STRENGTH = 100_000\t# Strength of the noise regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the image and VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert = transforms.Compose([\n",
    "\ttransforms.Resize(IMAGE_SIZE),\n",
    "\ttransforms.CenterCrop(IMAGE_SIZE),\n",
    "\ttransforms.ToTensor()\n",
    "])\n",
    "\n",
    "file = os.path.join('..', IMAGE_PATH)\n",
    "\n",
    "if NB_CHANNELS == 1:\n",
    "\ttarget = Image.open(file).convert('L')\n",
    "elif NB_CHANNELS <= 3:\n",
    "\ttarget = Image.open(file).convert('RGB')\n",
    "else:\n",
    "\ttarget = Image.open(file).convert('RGBA')\n",
    "\n",
    "target = convert(target) * 2 - 1\n",
    "\n",
    "if NB_CHANNELS == 2:\n",
    "\ttarget = target[:2]\n",
    "\n",
    "target = target.to(DEVICE).unsqueeze(0).detach().requires_grad_(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = tv.models.vgg16(weights = tv.models.VGG16_Weights.DEFAULT).to(DEVICE)\n",
    "vgg16 = nn.Sequential(*list(vgg16.features.children()))\n",
    "vgg16.eval()\n",
    "vgg16.requires_grad_(False)\n",
    "\n",
    "vgg16_transform = tv.models.VGG16_Weights.DEFAULT.transforms()\n",
    "\n",
    "\n",
    "def vgg16_forward(x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "\tx = (x + 1.0) / 2.0\n",
    "\tx = vgg16_transform(x)\n",
    "\treturn vgg16(x)\n",
    "\n",
    "\n",
    "target_features = vgg16_forward(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = model.gen_w(MEAN_W_SAMPLES)\n",
    "w_mean = ws.mean(0, keepdims = True).detach().requires_grad_(False)\n",
    "w_std = ((ws - w_mean).square().sum() / MEAN_W_SAMPLES).sqrt().item()\n",
    "\n",
    "w = w_mean.clone().requires_grad_(True)\n",
    "noise = model.gen_noise(1)\n",
    "noise = [n.requires_grad_(True) for n in noise]\n",
    "\n",
    "optimizer = torch.optim.Adam([w] + noise, lr = 0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(NB_STEPS):\n",
    "\n",
    "\toptimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "\tlr_start = min(1.0, step / WARMUP_STEPS)\n",
    "\tlr_end = min(1.0, (NB_STEPS - step) / COOLDOWN_STEPS)\n",
    "\tlr_end = 0.5 - 0.5 * np.cos(lr_end * np.pi)\n",
    "\tlr = LEARNING_RATE * lr_start * lr_end\n",
    "\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\tparam_group['lr'] = lr\n",
    "\n",
    "\tw_noise_scale = w_std * W_NOISE_SCALE * max(0.0, 1.0 - step / W_NOISE_STEPS) ** 2\n",
    "\tw = w + w_noise_scale * torch.randn_like(w)\n",
    "\n",
    "\tgen_image = model.synthesis(w, noise)\n",
    "\tgen_features = vgg16_forward(gen_image)\n",
    "\n",
    "\tmain_loss = (gen_features - target_features).square().sum()\n",
    "\tnoise_reg = 0.0\n",
    "\n",
    "\tfor n in noise:\n",
    "\n",
    "\t\ttemp_n = n.clone()\n",
    "\n",
    "\t\twhile True:\n",
    "\n",
    "\t\t\tnoise_reg = noise_reg + (temp_n * torch.roll(temp_n, shifts = 1, dims = 2)).mean().square()\n",
    "\t\t\tnoise_reg = noise_reg + (temp_n * torch.roll(temp_n, shifts = 1, dims = 3)).mean().square()\n",
    "\n",
    "\t\t\tif temp_n.shape[2] <= 8:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\ttemp_n = nn.functional.avg_pool2d(temp_n, kernel_size = 2)\n",
    "\n",
    "\tloss = main_loss + NOISE_REG_STRENGTH * noise_reg\n",
    "\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i in range(len(noise)):\n",
    "\t\t\tnoise[i] = noise[i] - noise[i].mean()\n",
    "\t\t\tnoise[i] = noise[i] * noise[i].square().mean().rsqrt()\n",
    "\n",
    "\tsave_image = utils.denormalize(gen_image.detach().squeeze(0))\n",
    "\n",
    "\tif not os.path.exists(os.path.join('..', SAVE_DIR)):\n",
    "\t\tos.makedirs(os.path.join('..', SAVE_DIR))\n",
    "\n",
    "\tImage.fromarray(save_image).save(os.path.join('..', SAVE_DIR, f'{step}.png'))\n",
    "\n",
    "\tprint(f'Steps: {step:,} / {NB_STEPS}  |  Loss: {main_loss.item():.3f}  |  Noise Regularisation: {noise_reg.item():.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
